




stopcode that was hit.   get  out  your  monitor  fiche  and

figure out why it got there.  bear in mind a stopcode can be

caused by software or hardware.  if sav30 and 30  have  same

contents,  normal  case  is that base took system down - see

key620 (output in 16 bit format) and first 16  bit  byte  is

reason  base  took  system down.  abnormal case is that base

wasn't running - this is detected by sav30 and 30 containing

a  "1"  (operations  is told to put a 1 in 30 to take system

down, but occasionally they may put something else in).   if

sav30 and 30 are different, then operator deposited non-zero

value in 30.  remember that sav30 is  normally  used  during

system  operation  as a normal instruction location, just as

crshac  is;  i.e.,  the  monitor  overwrites  code  to  save

contents of location 30 and the acs.  therefore, if sav30 or

the acs look like instructions, they  probably  are.   check

the  locations in the monitor to see if the data matches the

instructions that normally reside there.  if  it  does,  the

system has not successfully completed the crash code.  if it

is not read bad key  from  host  or  input  ring  processing

timeout,  see  if  the  base  has dumped itself into memory.

this can be determined by looking at location filser in  the

monitor.  if bits 32-35 are all ones, then it has.  save the

crash for the network  people  to  look  at.   by  1)monitor

telling  base in initial message that it has more ports than

base has (configuration problem with  value  of  portn),  2)

monitor  sending  bad  message  type to base (look at output











                                                    PAGE   2



ring to base (oring) to see if all  messages  are  legal  3)

monitor  sending  port  number  out  of  range to base (same

procedure as sending bad message type).  hung, although  apr

level  is  refreshing  the key.  this usually indicates that

scheduler level is not running.  see section on hung crashes

(look at thstim, etc.) also appears if host stops refreshing

key or if host places bad value for key into key620.



don't forget -  the  main  drive  behind  analysis  of  hung

crashes  is  to  find  out what the machine was doing, i.e.,

find the most recent pc the machine  was  known  to  be  at.

this  motive  is very easy to forget as one gets lost in the

intricacies of the crash.  don't  spend  too  much  time  on

tangents  -  find  out what the machine was doing.  check pi

channels in progress, look at pcs in chn for all  values  of

n,  look  at  device  conis  to  make  sure  device  is  not

interrupting continuously  on  wrong  channel  or  on  right

channel  but  service  routine is not paying attention (line

printer service turns off its conso when it  thinks  printer

is not supposed to be running).



first see how long its been since we  ran  scnser  (and  the

scheduler).    open  scnser  and  compare  thstim  with  the

contents of uptime.  if this comparison is not  very  close,

problem  was  we didn't get to scheduler any more.  look for

loop at pi level (see pists  to  see  if  any  pis  were  in











                                                    PAGE   3



progress),  and  if  not,  look for loop in scheduler level.

use stacks and  contents  of  acs  to  pinpoint  last  known

location.   remember  -  object  is  to  find out as much as

possible about last known location that monitor  was.   look

at  all jbtsts words with run bit on if channel 7 is running

normally.  see if they are all stuck in one state (ac  wait,

for example when chkpnt or stream accounting dies).  look at

parpc to see if machine was  processing  user  prity  error,

which can take so long sometimes that base takes us down for

input ring processing timeout (code 7).  look at unihng  for

all  disk  units  to make sure a disk isn't hung - sometimes

certain programs and/or jobs can still run  if  they  aren't

using  files  or  pages  on  the hung disk unit, while other

programs/and or  jobs  will  be  hung.also  look  for  large

numbers of errors



sometimes  looking  to  see  what  acs  were  at  last  disk

interrupt will yield information about a monitor loop.



if swapper hangs, see if siptot disagrees  with  the  actual

number  of  pages being swapped in.  if siptot ever gets off

by 1 or more (which it has been known to do) and  sipqta  is

sufficiently  low,  swapper  will stop doing anything, opers

will probably crash system as being hung.



if dump indicates that oprs hit reset  before  saving  dump,











                                                    PAGE   4



sometimes  they  couldn't  get the dump any other way.  call

oprs and see if they filled out a  hang  sheet.   one  thing

that  happens  sometimes  is  that  processor makes a memory

request and memory sends ack but no data  -  processor  will

just hang in this case.  if pi system is indicated as on, pi

requests are up, all pi channels were indicated on,  but  no

pi in progress were on, then processor is hung.  



the system will hang if chkpnt dies.  there are several ways

to  recognize  this condition.  are not turned on be able to

continue  running  until  they  perform  an  operation  that

requires  a  stream record to be written) look for many hard

disk errors on one or more  units  (unihct)  or  hung  units

(unihng) if chkpnt is still running.  this is likely if many

jobs are stuck in sw wait, or many jobs have mrq on.  to see

for  sure,  check  unihng for hung disk units.  if there are

one or more units with high hung counts,  this  is  probably

what was causing system to hang.  sometimes the parity error

scan takes so long that the system looks hung and  the  base

or the operators take it down.  check parpc, parspr, partot,

and the most recent pc that can  be  found  to  see  if  the

system  is  processing  a  parity error.  beware of the fact

that on nxm crashes the processor  can  actually  execute  a

zero   before   the  nxm  interrupt  occurs.   this  can  be

confusing.  on the ki, there is no hope of knowning  exactly

what  happened,  but  on the kl, examination of erasts shows











                                                    PAGE   5



the physical address (not virtual) that caused the nxm.   if

this  is  within the range of the memory indicated by memsiz

and pgynxm bits (watch out for holes in memory, don't assume

all  memory  from  0  to  c(memsiz)  is  there)  then  it is

certainly a hardware crash.  otherwise, look for a bad  page

map  pointer  in the current upt or ept.  on the ki, one has

to deduce the location of the nxm by the contents of the acs

and  the pc at interrupt time .  this must be done with care

- the pc at the time of the interrupt isn't necessarily  the

region  where  the  nxm  occurred.   one must investiage all

ending sections of interrupt and trap code, which  may  have

nxmed just before dismissing.  hanging around somewhere is a

file chunk.cmd.  if used properly, one can sometimes  obtain

information  about  why a system went down that is otherwise

unavailable.  setup p1 and p2 as start and end byte pointers

(if  p2 is zero will type out to end of chunk list) and type

out tty  chunk  free  pool.   this  will  sometimes  contain

messages from the operator about the system, cty output that

the operator lost or forgot to tell operating systems group,

etc.  



search pcbs for disk address of a rib if you have  a  desire

to  see a rib that was recently dealt with - it may still be

in core.



look at the accounting buffer or the stream accounting files











                                                    PAGE   6



(*.sat on un1 or billing10) for clues regarding who ran what

programs.  bear in mind  that  not  all  runs  cause  stream

records  to  be  written,  and that it is possible that some

stream data got lost by being in chkpnt's  data  buffers  at

the time of the crash.



don't overlook the locations on the stack  higher  than  the

address of the current value of the stack pointer.  this can

be  very  helpful  in  reconstructing  the  path  that   was

traversed by the machine in getting to where it died.



sometimes it is possible to deduce  the  execution  path  by

careful  examination  of  the  acs.  the key is to recognize

what the data in the ac is (ldb? clock coni word? ddb? etc.)



occasionally there will be a crash where  it  looks  like  a

disk  transfer brought in the wrong page.  in these crashes,

it is important to check to see  if  the  i/o  is  still  in

progress.   if  the  job  was  awakened  too  early for some

reason, it will see the old contents of the page and not the

new  one,  and disaster usually results.  usually, processor

executing instructions incorrectly and confused  crash  dump

analyst are indistinguishable.



this dt happen frequently in out monitor  anymore,  but

watch  out for pi level smashing lower level acs.  always be











                                                    PAGE   7



suspicious if the monitor crashes and the pc is close to one

that is contained in an interrupt jsr cell.  when looking at

a crash, always make sure you have the correct acs  for  the

place  you are looking at.  for example, a page fault for an

exec page will stack the acs and then die in the page  fault

code.   always look at the previous context acs in this case

and similar cases, or many hours of  confusion  can  result.

for  ki10  crashes, to look at previous acs one must go into

"code mode" and set contents of $i ("altmode i") to  zero  -

lh of $i represents the simulated user iot bit.  this causes

filddt to look at the ac stack instead of indicated user  ac

block.   on  kl10 crashes, previous context acs are fixed by

the hardware and are not affected by either  the  real  user

iot  flag  in  the  actual  machine  or the simulated one in

filddt.  in  crashes  where  freecore  allocation  has  been

messed  up (giving back free core that was already free, two

users of same freecore) try to recognize what data structure

is likely using the core by the contents of the core.  refer

to a chart of uses of monitor free core and it may occur  to

you  what  is  being  used.   (disk ddbs, block i/o buffers,

secondary pcbs, club tables, spts all come out of freecore).

check  to  make  sure all callers of getwds, givwds, get4wd,

and giv4wd are allocating as much free core as they  try  to

give back.  sometimes confusing information found in a crash

dump can be attributed to the monitor being overwritten.  if

you find yourself suspecting that the processor has executed











                                                    PAGE   8



an instruction or instructions incorrectly,  check  to  make

sure  the  code in the monitor hasn't been clobbered.  to do

this, use the xpand  program  to  convert  both  the  source

monitor  and  the  crash  dump  into  expanded  form, and do

selective filcoms between the two to see if any instructions

have  been modified.  the filcom must be selective, since in

general code and data are  interleaved  in  virtual/physical

memory  in  the  monitor.  it is good to start the filcom at

location filser (this skips some actual code in  common  and

commod,  so  beware),  and end at the end of the patch area,

location pat.  it is usually advisable to  do  two  seperate

filcoms,  one  between  filser  and  crshac, and one between

crshac+20*10 and pat.  this is  because  there  is  code  at

crshac  that  is  overwritten in a normal crash dump with ac

data.  specify the source monitor file first, then the crash

monitor  file.  after doing the filcoms into files, edit the

files and search for the following  fancy  string:  <beg  of

line>6<any  char><tab><1,2,3,4,5,6>.  edit10 string for this

is:?9?/?/?/?/?/?/<tab>?0123456?0.  this will find all  words

in   the  source  monitor  file  that  are  likely  to  have

instructions in them  that  have  been  modified  since  the

system  came  up.   note that some of these changes are ok -

jen instructions that have now become part of a conso chain,

etc.   just  take the addresses with the differences and see

whether or not it is  reasonable  that  they  have  changed.

note  that  this  procedure  skips  some  code - it can only











                                                    PAGE   9



ensure that there is an instruction that  was  changed,  not

that  one  was  not.  also note that the monitor's data base

being overwritten is also  a  source  of  strange  problems,

although  there  is  no way in many cases of knowing what is

supposed to be there unless some other part of the data base

suggests  some  former contents.  the important thing to use

as a clue is the address and data of  the  overwritten  cell

(cells).   is the data pattern a recognizable one? if so, it

might help isolate which data structure the  monitor  thinks

it  is  writing  into.   was  data  anded to memory, ored to

memory, movem ed to memory? which bits  are  involved?  does

the  address  appear  in any acs? don't forget to look in ac

save areas (the ac stack, uptxac for jobs with their upts in

core  and below 256k).  are there any other crashes with the

same data or addresses smashed? if so, is there a pattern to

which job or jobs are logged in, which programs are running?

a common monitor failure is using ac j without setting it up

-  and  a  common  value for j to be if it is not setup is a

controller data block address.  look  at  scnncb+chnkon  for

the  controller data block addresses, and subtract the value

from the smashed address.  if this turns out to be the exact

address  of  word 0 of a jbt table, you can then look at the

references to the jbt table to see which one is the culprit.



when damage occurrs to core somewhere, make sure no  channel

command  lists  point  to  damaged  area  (could  be  broken











                                                    PAGE  10



channel).  also broken base could cause this - look for bits

23-35 all 1.  if suspect base wrote the data, look at output

ring for block i/o requests and see if the  request  address

bears  any  relation to the addresses that were overwritten.

always make sure you are looking at an exact listing of code

that  is  suspected of causing a crash, or that you are very

very aware that what you are looking at  is  not  the  exact

code running at the time of the crash.  many times a bug has

been overlooked because the older or newer  version  of  the

code did not have a bug but the running version did.



don't believe that a pc looking thing on the stack is  a  pc

unless  the stack pointer and the code indicate that a pc is

there (example - uuocon skips over  a  word  on  the  stack,

leaving what could be a word looking like a pc from the last

page fault)



always think twice  before  chalking  a  crash  dump  up  to

processor  failure.   sometimes  analyzing  a running "dead"

system can save some time.  here are some techniques to  use

on  a system that is stil running but is about to be crashed

because it is not getting any useful work  done.   login  to

system  37  and get a copy of the monitor that is running on

the sick system into core and into ddt.  ask the operator to

examine  key locations that you want to see to determine the

problem.  for example, see what  location  strddb+strtal  is











                                                    PAGE  11



and  ask  the  operator  to  examine  it  to see if system's

problems are due to disk space.  this message comes out when

there is not enough disk to allocate context pages, when the

system is out of job slots, when  the  system  doesn't  have

enough contiguous free core to create a new tty ddb, or when

login kicks a user off because it is reserving job slots for

gan 3.  this can be caused by a large variety of things, but

check to see if any one particular job is  getting  all  the

runtime  in the system.  sometimes (pj)pam goes into a loop.

sometimes it is a job that is abusing/using schprv.  general

strategy  is  to  find  out where the system is spending its

time.  sometimes the field  engineers  are  testing  a  disk

drive  off line through the storage control, which will slow

down disk transfers somewhat, although they  should  not  be

doing this on customer systems.  if a job can't be hung, see

if it still has command bits  set  for  it  -  look  at  its

jbtsts, see if comcnt is still non-zero.  try to look at the

job's context pages and see if its  pc  is  in  the  monitor

(will keep control-c from happening).
























   ld6I¯